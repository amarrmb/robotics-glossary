---
title: Jetson Orin
description: NVIDIA's edge AI platform for robotics (2022-2025), now in long-term support
sidebar:
  badge:
    text: Practical
    variant: success
---

import { Aside, Card, CardGrid, LinkCard, Tabs, TabItem } from '@astrojs/starlight/components';

<span class="level-badge practical">Practical</span>

**Jetson Orin** was NVIDIA's flagship edge AI computing platform from 2022-2025, designed for robotics, autonomous machines, and AI applications requiring real-time performance at the edge. While superseded by [Jetson Thor](/robotics-glossary/hardware/compute/jetson-thor/) for new high-performance designs, Orin remains widely deployed and fully supported.

<Aside type="note" title="Status: Long-Term Support">
Orin entered LTS in late 2025. JetPack 6.x will receive security updates through 2030. For new projects requiring >300 TOPS, consider [Jetson Thor](/robotics-glossary/hardware/compute/jetson-thor/).
</Aside>

## The Orin Family

<Tabs>
  <TabItem label="AGX Orin">
    **Jetson AGX Orin** — The flagship module

    | Spec | Value |
    |------|-------|
    | AI Performance | 275 TOPS (INT8) |
    | GPU | Ampere, 2048 CUDA cores, 64 Tensor cores |
    | CPU | 12-core Arm Cortex-A78AE |
    | Memory | 32GB or 64GB LPDDR5 |
    | Power | 15W - 60W (configurable) |
    | Status | Long-term support |

    **Best for**: Production deployments, multi-sensor fusion, existing Orin-based designs
  </TabItem>

  <TabItem label="Orin NX">
    **Jetson Orin NX** — High performance, compact form factor

    | Spec | 16GB | 8GB |
    |------|------|-----|
    | AI Performance | 100 TOPS | 70 TOPS |
    | GPU | 1024 CUDA cores | 1024 CUDA cores |
    | CPU | 8-core Arm Cortex-A78AE | 6-core |
    | Memory | 16GB LPDDR5 | 8GB LPDDR5 |
    | Power | 10W - 25W | 10W - 20W |
    | Status | Active production |

    **Best for**: Delivery robots, drones, industrial AMRs, cost-sensitive applications
  </TabItem>

  <TabItem label="Orin Nano">
    **Jetson Orin Nano** — Entry-level AI at the edge

    | Spec | 8GB | 4GB |
    |------|-----|-----|
    | AI Performance | 40 TOPS | 20 TOPS |
    | GPU | 1024 CUDA cores | 512 CUDA cores |
    | CPU | 6-core Arm Cortex-A78AE | 6-core |
    | Memory | 8GB LPDDR5 | 4GB LPDDR5 |
    | Power | 7W - 15W | 5W - 10W |
    | Status | Active production |

    **Best for**: Entry-level robots, education, prototyping, high-volume deployments
  </TabItem>
</Tabs>

## Orin vs Thor: Which to Choose?

| Consideration | Choose Orin | Choose Thor |
|--------------|-------------|-------------|
| AI Performance needed | &lt;300 TOPS | 800+ TOPS |
| Budget | Cost-sensitive | Performance-critical |
| Existing design | Migrating from Xavier | New humanoid/advanced robot |
| Transformer workloads | Limited | Native Transformer Engine |
| Timeline | Production now | 2026+ deployment |

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    Jetson Orin SoC                          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────────────────────┐  │
│  │   Arm CPU       │  │        NVIDIA Ampere GPU        │  │
│  │  Cortex-A78AE   │  │  ┌─────────┐  ┌─────────────┐  │  │
│  │  Up to 12 cores │  │  │  CUDA   │  │   Tensor    │  │  │
│  │                 │  │  │  Cores  │  │   Cores     │  │  │
│  └─────────────────┘  │  └─────────┘  └─────────────┘  │  │
│                       └─────────────────────────────────┘  │
│  ┌─────────────────┐  ┌─────────────────────────────────┐  │
│  │  Deep Learning  │  │         Video Engines           │  │
│  │  Accelerator    │  │  NVENC │ NVDEC │ JPEG │ OFA    │  │
│  │  (DLA x2)       │  │                                 │  │
│  └─────────────────┘  └─────────────────────────────────┘  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Memory: LPDDR5 (256-bit)               │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

### Key Components

- **Ampere GPU**: CUDA and Tensor cores for parallel compute and AI inference
- **DLA (Deep Learning Accelerator)**: Dedicated AI inference engines (2x on AGX Orin)
- **PVA (Programmable Vision Accelerator)**: Computer vision preprocessing
- **Video Engines**: Hardware encode/decode for camera streams

## Software Stack

<Tabs>
  <TabItem label="JetPack 6.x (LTS)">
    ```bash
    # JetPack 6.2 - Long Term Support (Recommended for Orin)
    # Ubuntu 22.04, CUDA 12.2, TensorRT 8.6
    sudo apt update
    sudo apt install nvidia-jetpack
    ```
    Recommended for production Orin deployments requiring stability.
  </TabItem>
  <TabItem label="JetPack 7.0">
    ```bash
    # JetPack 7.0 - Latest features (Primary target: Thor)
    # Ubuntu 24.04, CUDA 12.8, TensorRT 10.2
    # Orin NX/Nano: Full support
    # AGX Orin: Limited support, verify compatibility
    ```
    Use only if you need latest Isaac ROS 4.0 features on Orin NX/Nano.
  </TabItem>
</Tabs>

```
┌─────────────────────────────────────────┐
│           Your Application              │
├─────────────────────────────────────────┤
│  Isaac ROS │ ROS 2 │ DeepStream │ TAO  │
├─────────────────────────────────────────┤
│  TensorRT │ cuDNN │ CUDA │ OpenCV      │
├─────────────────────────────────────────┤
│           JetPack SDK (L4T)             │
├─────────────────────────────────────────┤
│        Linux Kernel + Drivers           │
└─────────────────────────────────────────┘
```

## Getting Started

### 1. Flash the Device

```bash
# Using NVIDIA SDK Manager (recommended)
# Or command line:
sudo ./flash.sh jetson-agx-orin-devkit internal
```

### 2. Install JetPack Components

```bash
sudo apt update
sudo apt install nvidia-jetpack
```

### 3. Verify Installation

```bash
# Check CUDA
nvcc --version

# Check TensorRT
dpkg -l | grep tensorrt

# Monitor system
tegrastats
```

## Power Management

Orin supports multiple power modes via `nvpmodel`:

```bash
# List available modes
sudo nvpmodel -q --verbose

# Set to max performance (AGX Orin)
sudo nvpmodel -m 0  # MAXN: 60W

# Set to power-efficient mode
sudo nvpmodel -m 3  # 30W

# Maximize clocks (for benchmarking)
sudo jetson_clocks
```

| Mode | AGX Orin Power | Use Case |
|------|----------------|----------|
| MAXN | 60W | Maximum performance |
| 50W | 50W | High performance |
| 30W | 30W | Balanced |
| 15W | 15W | Power-constrained |

## Related Terms

<CardGrid>
  <LinkCard
    title="Jetson Thor"
    description="Next-gen platform for humanoid robots (800+ TOPS)"
    href="/robotics-glossary/hardware/compute/jetson-thor/"
  />
  <LinkCard
    title="Isaac ROS"
    description="GPU-accelerated ROS 2 packages for Jetson"
    href="/robotics-glossary/software/isaac/isaac-ros/"
  />
  <LinkCard
    title="DGX Spark"
    description="Desktop AI supercomputer for development"
    href="/robotics-glossary/hardware/compute/dgx-spark/"
  />
</CardGrid>

## Learn More

- [Jetson Orin Official Page](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
- [JetPack 6.x LTS Documentation](https://developer.nvidia.com/embedded/jetpack-6)
- [Orin → Thor Migration Guide](https://developer.nvidia.com/embedded/jetson-thor-migration)
- [Jetson Developer Forums](https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/)

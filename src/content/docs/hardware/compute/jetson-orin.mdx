---
title: Jetson Orin
description: NVIDIA's edge AI platform for robotics, in active production with LTS through 2032
last_validated: 2026-01-21
sidebar:
  badge:
    text: Practical
    variant: success
---

import { Aside, Card, CardGrid, LinkCard, Tabs, TabItem } from '@astrojs/starlight/components';

<span class="level-badge practical">Practical</span>

**Jetson Orin** is NVIDIA's edge AI computing platform launched in 2022, designed for robotics, autonomous machines, and AI applications requiring real-time performance at the edge. All Orin modules remain in active production with support extended through January 2032.

<Aside type="note" title="Status: Active Production">
All Orin modules are in active production with LTS extended through January 2032. JetPack 6.2 introduced "Super Mode" enabling higher performance on NX and Nano modules.
</Aside>

## The Orin Family

<Tabs>
  <TabItem label="AGX Orin">
    **Jetson AGX Orin** — The flagship module

    | Spec | 64GB | 32GB |
    |------|------|------|
    | AI Performance | 275 TOPS (INT8) | 200 TOPS (INT8) |
    | GPU | 2048 CUDA cores, 64 Tensor cores | 1792 CUDA cores, 56 Tensor cores |
    | CPU | 12-core Arm Cortex-A78AE | 8-core Arm Cortex-A78AE |
    | Memory | 64GB LPDDR5 | 32GB LPDDR5 |
    | Power | 15W - 60W | 15W - 40W |
    | Status | Long-term support | Long-term support |

    **Best for**: Production deployments, multi-sensor fusion, existing Orin-based designs
  </TabItem>

  <TabItem label="Orin NX">
    **Jetson Orin NX** — High performance, compact form factor

    | Spec | 16GB | 8GB |
    |------|------|-----|
    | AI Performance | 100 TOPS (157 w/ Super Mode) | 70 TOPS (117 w/ Super Mode) |
    | GPU | 1024 CUDA cores | 1024 CUDA cores |
    | CPU | 8-core Arm Cortex-A78AE | 6-core |
    | Memory | 16GB LPDDR5 | 8GB LPDDR5 |
    | Power | 10W - 25W (40W Super Mode) | 10W - 20W |
    | Status | Active production |

    **Best for**: Delivery robots, drones, industrial AMRs, cost-sensitive applications
  </TabItem>

  <TabItem label="Orin Nano">
    **Jetson Orin Nano** — Entry-level AI at the edge

    | Spec | 8GB | 4GB |
    |------|-----|-----|
    | AI Performance | 40 TOPS (67 w/ Super Mode) | 20 TOPS (32 w/ Super Mode) |
    | GPU | 1024 CUDA cores | 512 CUDA cores |
    | CPU | 6-core Arm Cortex-A78AE | 6-core |
    | Memory | 8GB LPDDR5 | 4GB LPDDR5 |
    | Power | 7W - 15W (25W Super Mode) | 5W - 10W |
    | Status | Active production |

    **Best for**: Entry-level robots, education, prototyping, high-volume deployments
  </TabItem>
</Tabs>

## Orin vs Thor: Which to Choose?

| Consideration | Choose Orin | Choose Thor |
|--------------|-------------|-------------|
| AI Performance needed | &lt;300 TOPS | 2,070 TFLOPS (FP4) |
| Budget | Cost-sensitive | Performance-critical ($3,499 dev kit) |
| Existing design | Migrating from Xavier | New humanoid/advanced robot |
| Transformer workloads | Limited | Native Transformer Engine |
| Memory | Up to 64GB | 128GB LPDDR5X |

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    Jetson Orin SoC                          │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────────────────────┐  │
│  │   Arm CPU       │  │        NVIDIA Ampere GPU        │  │
│  │  Cortex-A78AE   │  │  ┌─────────┐  ┌─────────────┐  │  │
│  │  Up to 12 cores │  │  │  CUDA   │  │   Tensor    │  │  │
│  │                 │  │  │  Cores  │  │   Cores     │  │  │
│  └─────────────────┘  │  └─────────┘  └─────────────┘  │  │
│                       └─────────────────────────────────┘  │
│  ┌─────────────────┐  ┌─────────────────────────────────┐  │
│  │  Deep Learning  │  │         Video Engines           │  │
│  │  Accelerator    │  │  NVENC │ NVDEC │ JPEG │ OFA    │  │
│  │  (DLA x2)       │  │                                 │  │
│  └─────────────────┘  └─────────────────────────────────┘  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Memory: LPDDR5 (256-bit)               │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

### Key Components

- **Ampere GPU**: CUDA and Tensor cores for parallel compute and AI inference
- **DLA (Deep Learning Accelerator)**: Dedicated AI inference engines (2x on AGX Orin)
- **PVA (Programmable Vision Accelerator)**: Computer vision preprocessing
- **Video Engines**: Hardware encode/decode for camera streams

## Software Stack

<Tabs>
  <TabItem label="JetPack 6.x (LTS)">
    ```bash
    # JetPack 6.2.1 - Long Term Support (Recommended for Orin)
    # Ubuntu 22.04, CUDA 12.6, TensorRT 10.3, cuDNN 9.3
    sudo apt update
    sudo apt install nvidia-jetpack
    ```
    Recommended for production Orin deployments requiring stability.
  </TabItem>
  <TabItem label="JetPack 7.x">
    ```bash
    # JetPack 7.x - Latest features (Primary target: Thor)
    # Ubuntu 24.04, CUDA 13.0, Linux Kernel 6.8
    # Orin support expected in JetPack 7.2 (Q1 2026)
    ```
    Primary target is Jetson Thor; Orin support expected in JetPack 7.2.
  </TabItem>
</Tabs>

```
┌─────────────────────────────────────────┐
│           Your Application              │
├─────────────────────────────────────────┤
│  Isaac ROS │ ROS 2 │ DeepStream │ TAO  │
├─────────────────────────────────────────┤
│  TensorRT │ cuDNN │ CUDA │ OpenCV      │
├─────────────────────────────────────────┤
│           JetPack SDK (L4T)             │
├─────────────────────────────────────────┤
│        Linux Kernel + Drivers           │
└─────────────────────────────────────────┘
```

## Getting Started

### 1. Flash the Device

```bash
# Using NVIDIA SDK Manager (recommended)
# Or command line:
sudo ./flash.sh jetson-agx-orin-devkit internal
```

### 2. Install JetPack Components

```bash
sudo apt update
sudo apt install nvidia-jetpack
```

### 3. Verify Installation

```bash
# Check CUDA
nvcc --version

# Check TensorRT
dpkg -l | grep tensorrt

# Monitor system
tegrastats
```

## Power Management

Orin supports multiple power modes via `nvpmodel`:

```bash
# List available modes
sudo nvpmodel -q --verbose

# Set to max performance (AGX Orin)
sudo nvpmodel -m 0  # MAXN: 60W

# Set to power-efficient mode
sudo nvpmodel -m 3  # 30W

# Maximize clocks (for benchmarking)
sudo jetson_clocks
```

| Mode | AGX Orin Power | Use Case |
|------|----------------|----------|
| MAXN | 60W | Maximum performance |
| 50W | 50W | High performance |
| 30W | 30W | Balanced |
| 15W | 15W | Power-constrained |

## Related Terms

<CardGrid>
  <LinkCard
    title="Jetson Thor"
    description="Next-gen platform for humanoid robots (2,070 TFLOPS)"
    href="/robotics-glossary/hardware/compute/jetson-thor/"
  />
  <LinkCard
    title="Isaac ROS"
    description="GPU-accelerated ROS 2 packages for Jetson"
    href="/robotics-glossary/software/isaac/isaac-ros/"
  />
  <LinkCard
    title="DGX Spark"
    description="Desktop AI supercomputer for development"
    href="/robotics-glossary/hardware/compute/dgx-spark/"
  />
</CardGrid>

## Learn More

- [Jetson Orin Official Page](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/)
- [JetPack 6.x LTS Documentation](https://developer.nvidia.com/embedded/jetpack-6)
- [Orin → Thor Migration Guide](https://developer.nvidia.com/embedded/jetson-thor-migration)
- [Jetson Developer Forums](https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/)

## Sources

- [NVIDIA Jetson Orin](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/) — Official specifications
- [NVIDIA AGX Orin Technical Brief](https://www.nvidia.com/content/dam/en-zz/Solutions/gtcf21/jetson-orin/nvidia-jetson-agx-orin-technical-brief.pdf) — AGX Orin 64GB/32GB variant specifications
- [NVIDIA JetPack SDK](https://developer.nvidia.com/embedded/jetpack) — Current JetPack 6.2.1 versions
- [JetPack 6.2 Super Mode Blog](https://developer.nvidia.com/blog/nvidia-jetpack-6-2-brings-super-mode-to-nvidia-jetson-orin-nano-and-jetson-orin-nx-modules/) — Super Mode feature details
- [Jetson Product Lifecycle](https://developer.nvidia.com/embedded/lifecycle) — LTS extended through January 2032
- [NVIDIA Newsroom - Jetson Thor](https://nvidianews.nvidia.com/news/nvidia-blackwell-powered-jetson-thor-now-available-accelerating-the-age-of-general-robotics) — Thor availability and specs
